{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "def loadLines(fileName, fields):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fileName (str): file to load\n",
    "        field (set<str>): fields to extract\n",
    "    Return:\n",
    "        dict<dict<str>>: the extracted fields for each line\n",
    "    \"\"\"\n",
    "    lines = {}\n",
    "\n",
    "    with open(fileName, 'r', encoding='iso-8859-1') as f:  # TODO: Solve Iso encoding pb !\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "\n",
    "            # Extract fields\n",
    "            lineObj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                lineObj[field] = values[i]\n",
    "\n",
    "            lines[lineObj['lineID']] = lineObj\n",
    "\n",
    "    return lines\n",
    "\n",
    "def loadConversations(fileName, fields, lines):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fileName (str): file to load\n",
    "        field (set<str>): fields to extract\n",
    "    Return:\n",
    "        dict<dict<str>>: the extracted fields for each line\n",
    "    \"\"\"\n",
    "    conversations = []\n",
    "\n",
    "    with open(fileName, 'r', encoding='iso-8859-1') as f:  # TODO: Solve Iso encoding pb !\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "\n",
    "            # Extract fields\n",
    "            convObj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                convObj[field] = values[i]\n",
    "\n",
    "            # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\")\n",
    "            lineIds = ast.literal_eval(convObj[\"utteranceIDs\"])\n",
    "\n",
    "            # Reassemble lines\n",
    "            convObj[\"lines\"] = []\n",
    "            for lineId in lineIds:\n",
    "                convObj[\"lines\"].append(lines[lineId])\n",
    "\n",
    "            conversations.append(convObj)\n",
    "\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIE_LINES_FIELDS = [\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n",
    "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\n",
    "\n",
    "#dirName = '~/Documents/Roboy/ss18_showmaster/DeepQA/data/cornell/'\n",
    "dirName = 'data/cornell/'\n",
    "\n",
    "\n",
    "lines = loadLines(os.path.join(dirName, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
    "conversations = loadConversations(os.path.join(dirName, \"movie_conversations.txt\"), MOVIE_CONVERSATIONS_FIELDS, lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the main code, the other stuff is just for better understanding...\n",
    "\n",
    "# create source-target touples from conversations, write them\n",
    "\n",
    "import re\n",
    "\n",
    "# training\n",
    "for j in range(0, int(0.8*len(conversations))):\n",
    "    x = conversations[j]\n",
    "    for i in range(0, len(x[\"lines\"])-1):\n",
    "        source = x[\"lines\"][i][\"text\"][:-1]  #remove line break\n",
    "        target = x[\"lines\"][i+1][\"text\"]     #keep line break\n",
    "        source = re.sub(\"[^\\w]\", \" \",  source).split()\n",
    "        target = re.sub(\"[^\\w]\", \" \",  target).split()\n",
    "        clean = open('data/cornell/train/data.txt', 'a', encoding='utf8') #open file to append\n",
    "        clean.write(\"\\t\".join([\" \".join(source), \" \".join(reversed(target))]))\n",
    "        clean.write('\\n')\n",
    "        clean.flush()\n",
    "#testing\n",
    "for j in range(int(0.8*len(conversations))+1,int(0.9*len(conversations))):\n",
    "    x = conversations[j]\n",
    "    for i in range(0, len(x[\"lines\"])-1):\n",
    "        source = x[\"lines\"][i][\"text\"][:-1]  #remove line break\n",
    "        target = x[\"lines\"][i+1][\"text\"]     #keep line break\n",
    "        source = re.sub(\"[^\\w]\", \" \",  source).split()\n",
    "        target = re.sub(\"[^\\w]\", \" \",  target).split()\n",
    "        clean = open('data/cornell/test/data.txt', 'a', encoding='utf8') #open file to append\n",
    "        clean.write(\"\\t\".join([\" \".join(source), \" \".join(reversed(target))]))\n",
    "        clean.write('\\n')\n",
    "        clean.flush()\n",
    "#dev\n",
    "for j in range(int(0.9*len(conversations))+1,len(conversations)):\n",
    "    x = conversations[j]\n",
    "    for i in range(0, len(x[\"lines\"])-1):\n",
    "        source = x[\"lines\"][i][\"text\"][:-1]  #remove line break\n",
    "        target = x[\"lines\"][i+1][\"text\"]     #keep line break\n",
    "        source = re.sub(\"[^\\w]\", \" \",  source).split()\n",
    "        target = re.sub(\"[^\\w]\", \" \",  target).split()\n",
    "        clean = open('data/cornell/dev/data.txt', 'a', encoding='utf8') #open file to append\n",
    "        clean.write(\"\\t\".join([\" \".join(source), \" \".join(reversed(target))]))\n",
    "        clean.write('\\n')\n",
    "        clean.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary from convo txt file\n",
    "\n",
    "#clear files first!\n",
    "\n",
    "\n",
    "for j in range(0, int(0.8*len(conversations))):\n",
    "    x = conversations[j]\n",
    "    for i in range(0, len(x[\"lines\"])-1):\n",
    "        source = x[\"lines\"][i][\"text\"][:-1]  #remove line break\n",
    "        target = x[\"lines\"][i+1][\"text\"]     #keep line break\n",
    "        source = re.sub(\"[^\\w]\", \" \",  source).split()\n",
    "        target = re.sub(\"[^\\w]\", \" \",  target).split()\n",
    "        clean = open('data/cornell/CleanConversation.txt', 'a', encoding='utf8') #open file to append\n",
    "        clean.write(\"\\t\".join([\" \".join(source), \" \".join(reversed(target))]))\n",
    "        clean.write('\\n')\n",
    "        #clean.write(source)\n",
    "        #clean.write(\"\\t\")\n",
    "        #clean.write(target)\n",
    "        clean.flush()\n",
    "        \n",
    "convo = open('data/cornell/CleanConversations.txt', 'r', encoding='utf8') #open file to append\n",
    "vocab = open('data/cornell/Vocab.txt', 'a', encoding='utf8') #open file to append\n",
    "\n",
    "import re\n",
    "\n",
    "for line in convo:\n",
    "    for word in line.split():\n",
    "        word_clean = re.sub(\"[^a-zA-Z'\\n]\",\"\", word)   #skip everything besides letter, 's, 're or apostrophe\n",
    "        #print(word)\n",
    "        vocab.write(word_clean)\n",
    "        vocab.write('\\n')\n",
    "        vocab.flush()   #force write to harddrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "#1\n",
    "output_file_path = 'data/cornell/Vocab_unique.txt'\n",
    "input_file_path = 'data/cornell/Vocab.txt'\n",
    "#2\n",
    "completed_lines_hash = set()\n",
    "#3\n",
    "output_file = open(output_file_path, \"w\")\n",
    "#4\n",
    "for line in open(input_file_path, \"r\"):\n",
    "  #5\n",
    "  hashValue = hashlib.md5(line.rstrip().encode('utf-8')).hexdigest()\n",
    "  #6\n",
    "  if hashValue not in completed_lines_hash:\n",
    "    output_file.write(line)\n",
    "    completed_lines_hash.add(hashValue)\n",
    "#7\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'character1ID': 'u0',\n",
       " 'character2ID': 'u2',\n",
       " 'movieID': 'm0',\n",
       " 'utteranceIDs': \"['L194', 'L195', 'L196', 'L197']\\n\",\n",
       " 'lines': [{'lineID': 'L194',\n",
       "   'characterID': 'u0',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'BIANCA',\n",
       "   'text': 'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\n'},\n",
       "  {'lineID': 'L195',\n",
       "   'characterID': 'u2',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'CAMERON',\n",
       "   'text': \"Well, I thought we'd start with pronunciation, if that's okay with you.\\n\"},\n",
       "  {'lineID': 'L196',\n",
       "   'characterID': 'u0',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'BIANCA',\n",
       "   'text': 'Not the hacking and gagging and spitting part.  Please.\\n'},\n",
       "  {'lineID': 'L197',\n",
       "   'characterID': 'u2',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'CAMERON',\n",
       "   'text': \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "x = conversations[2]\n",
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['character1ID', 'character2ID', 'lines', 'movieID', 'utteranceIDs']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[\"lines\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'L198'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"utteranceIDs\"][2:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'L199'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"utteranceIDs\"][10:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lineID': 'L202',\n",
       " 'characterID': 'u0',\n",
       " 'movieID': 'm0',\n",
       " 'character': 'BIANCA',\n",
       " 'text': \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\"}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"lines\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lineID': 'L200',\n",
       " 'characterID': 'u0',\n",
       " 'movieID': 'm0',\n",
       " 'character': 'BIANCA',\n",
       " 'text': \"No, no, it's my fault -- we didn't have a proper introduction ---\\n\"}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"lines\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, no, it's my fault -- we didn't have a proper introduction ---\\n\""
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"lines\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, no, it's my fault -- we didn't have a proper introduction ---\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"lines\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, no, it's my fault -- we didn't have a proper introduction ---\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"lines\"][0][\"text\"][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get into field shape\n",
    "\n",
    "#for _ in range(length):\n",
    "#    seq.append(str(random.randint(0, 9)))\n",
    "#fout.write(\"\\t\".join([\" \".join(seq), \" \".join(reversed(seq))]))\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "mystr = 'This is a string, with words!'\n",
    "wordList = re.sub(\"[^\\w]\", \" \",  mystr).split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'string', 'with', 'words']\n"
     ]
    }
   ],
   "source": [
    "print(wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
